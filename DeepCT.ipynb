{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DeepCT is written for tensorflow v1. To make this notebook work it has to be updated to v2 using the tensorflow update script.\n",
        "\n",
        "Furthermore the following things have to be changed manually:\n",
        "\n",
        "\n",
        "*   run_deepct.py\n",
        "  * Line 33: flags = tf.compat.v1.flags\n",
        "  * Line 112: tf.compat.v1.flags.DEFINE_string\n",
        "  * Line 118: tf.compat.v1.flags.DEFINE_string\n",
        "  * Line 124: tf.compat.v1.flags.DEFINE_string\n",
        "  * Line 130: tf.compat.v1.flags.DEFINE_string\n",
        "  * Line 795: hidden_size = bert_output_layer.shape[-1]\n",
        "  * Line 796: eq_length = bert_output_layer.shape[-2]\n",
        "* modeling.py\n",
        "  * Line 365: layer = tf.keras.layers.LayerNormalization(name=name,axis=-1,epsilon=1e-12,dtype=tf.float32)\n",
        "\n",
        "    return layer(input_tensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "HBAoO6Mon9O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AdeDZY/DeepCT.git"
      ],
      "metadata": {
        "id": "hoLvisPFpBfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkDt_eLEZB2G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MENY8oofhSIl"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "\n",
        "!tf_upgrade_v2 --intree /content/DeepCT --outtree /content/DeepCT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hprYj9YGAH_"
      },
      "source": [
        "# Convert the HuggingFace safetensor to a tensorflow ckpt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J1U6Qfe-Tpds"
      },
      "outputs": [],
      "source": [
        "var_map = (\n",
        "        (\"layer.\", \"layer_\"),\n",
        "        (\"word_embeddings.weight\", \"word_embeddings\"),\n",
        "        (\"position_embeddings.weight\", \"position_embeddings\"),\n",
        "        (\"token_type_embeddings.weight\", \"token_type_embeddings\"),\n",
        "        (\".\", \"/\"),\n",
        "        (\"LayerNorm/weight\", \"LayerNorm/gamma\"),\n",
        "        (\"LayerNorm/bias\", \"LayerNorm/beta\"),\n",
        "        (\"weight\", \"kernel\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E60v7duuRd4l"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4tmoZJrJap3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from transformers import BertForSequenceClassification, BertModel\n",
        "\n",
        "\n",
        "def to_tf_var_name(name: str):\n",
        "    for patt, repl in iter(var_map):\n",
        "        name = name.replace(patt, repl)\n",
        "\n",
        "    return \"{}\".format(name)\n",
        "\n",
        "def create_tf_var(tensor: np.ndarray, name: str, session: tf.Session):\n",
        "    tf_dtype = tf.dtypes.as_dtype(tensor.dtype)\n",
        "    tf_var = tf.get_variable(dtype=tf_dtype, shape=tensor.shape, name=name, initializer=tf.zeros_initializer())\n",
        "    session.run(tf.variables_initializer([tf_var]))\n",
        "    session.run(tf_var)\n",
        "    return tf_var\n",
        "\n",
        "\n",
        "def convert_pytorch_checkpoint_to_tf(model: BertModel, ckpt_dir: str, model_name: str):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model: BertModel Pytorch model instance to be converted\n",
        "        ckpt_dir: Tensorflow model directory\n",
        "        model_name: model name\n",
        "\n",
        "    Currently supported HF models:\n",
        "\n",
        "        - Y BertModel\n",
        "        - N BertForMaskedLM\n",
        "        - N BertForPreTraining\n",
        "        - N BertForMultipleChoice\n",
        "        - N BertForNextSentencePrediction\n",
        "        - N BertForSequenceClassification\n",
        "        - N BertForQuestionAnswering\n",
        "    \"\"\"\n",
        "\n",
        "    tensors_to_transpose = (\"dense.weight\", \"attention.self.query\", \"attention.self.key\", \"attention.self.value\")\n",
        "\n",
        "    # variable mapping\n",
        "    var_map = (\n",
        "        (\"layer.\", \"layer_\"),\n",
        "        (\"word_embeddings.weight\", \"word_embeddings\"),\n",
        "        (\"position_embeddings.weight\", \"position_embeddings\"),\n",
        "        (\"token_type_embeddings.weight\", \"token_type_embeddings\"),\n",
        "        (\".\", \"/\"),\n",
        "        (\"LayerNorm/weight\", \"LayerNorm/gamma\"),\n",
        "        (\"LayerNorm/bias\", \"LayerNorm/beta\"),\n",
        "        (\"weight\", \"kernel\"),\n",
        "    )\n",
        "\n",
        "    if not os.path.isdir(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    print(\"preprocess... \")\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as session:\n",
        "        for var_name in state_dict:\n",
        "            print(var_name)\n",
        "            if var_name in {'classifier.bias', 'classifier.weight'}:\n",
        "                print(f'skippin {var_name}')\n",
        "                continue\n",
        "            print(\"var_name\", var_name)\n",
        "            tf_name = to_tf_var_name(var_name)\n",
        "\n",
        "            # classification weight and bias\n",
        "            if tf_name == \"classifier/kernel\":\n",
        "                tf_name = \"output_weights\"\n",
        "            if tf_name == \"classifier/bias\":\n",
        "                tf_name = \"output_bias\"\n",
        "\n",
        "            print(tf_name)\n",
        "            torch_tensor = state_dict[var_name].numpy()\n",
        "            if any([x in var_name for x in tensors_to_transpose]):\n",
        "                torch_tensor = torch_tensor.T\n",
        "            tf_var = create_tf_var(tensor=torch_tensor, name=tf_name, session=session)\n",
        "            tf.keras.backend.set_value(tf_var, torch_tensor)\n",
        "            tf_weight = session.run(tf_var)\n",
        "            print(\"Successfully created {}: {}\".format(tf_name, np.allclose(tf_weight, torch_tensor)))\n",
        "\n",
        "        # save tensorflow checkpoint file\n",
        "        saver = tf.train.Saver(tf.trainable_variables())\n",
        "        saver.save(session, os.path.join(ckpt_dir, model_name.replace(\"-\", \"_\") + \".ckpt\"))\n",
        "\n",
        "\n",
        "def main(raw_args=None):\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained('merged-model')\n",
        "    #model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    for var_name in model.state_dict():\n",
        "        print(var_name)\n",
        "\n",
        "    convert_pytorch_checkpoint_to_tf(model=model,\n",
        "                                     ckpt_dir=\"merged-model\",\n",
        "                                     model_name=\"convertedModel\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc_O6pCEW7ff"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data file can be downloaded from https://boston.lti.cs.cmu.edu/appendices/arXiv2019-DeepCT-Zhuyun-Dai/data/.\n",
        "\n",
        "If BERT is trained the vocab and the config file can be the one from the google bert-base-uncased network.\n",
        "\n",
        "The init checkpoint is the path to the tensorflow network."
      ],
      "metadata": {
        "id": "oV1aO9depKGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZnQtcxEeYNX"
      },
      "outputs": [],
      "source": [
        "project_path = \"\"\n",
        "\n",
        "!python /DeepCT/run_deepct.py  \\\n",
        "  --task_name=marcodoc \\\n",
        "  --do_train=true \\\n",
        "  --do_eval=false \\\n",
        "  --do_predict=false \\\n",
        "  --data_dir=path_to_data \\\n",
        "  --vocab_file=path_to_vocab \\\n",
        "  --bert_config_file=path_to_config \\\n",
        "  --init_checkpoint=path_to_converted_model \\\n",
        "  --max_seq_length=128 \\\n",
        "  --train_batch_size=16 \\\n",
        "  --learning_rate=2e-5 \\\n",
        "  --num_train_epochs=1.0 \\\n",
        "  --recall_field=title \\\n",
        "  --save_checkpoints_steps=10000 \\\n",
        "  --output_dir=DeepCT_Trained"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-hprYj9YGAH_",
        "9AlX7tBkBWLV",
        "Bc_O6pCEW7ff",
        "cxx87BysU7LD"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}